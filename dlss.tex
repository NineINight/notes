\documentclass[]{article}
\usepackage{etex}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{color}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage[lined]{algorithm2e}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage[pdftex,
  pdfauthor={Michael Noukhovitch},
  pdftitle={},
  pdfsubject={Lecture notes from },
  pdfproducer={LaTeX},
  pdfcreator={pdflatex}]{hyperref}

\usepackage{cleveref}
\usepackage{enumitem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem{ex}{Example}[section]
\newtheorem*{theorem}{Theorem}

\setlength{\marginparwidth}{1.5in}
\setlength{\algomargin}{0.75em}

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}

\definecolor{darkish-blue}{RGB}{25,103,185}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=darkish-blue,
    filecolor=darkish-blue,
    linkcolor=darkish-blue,
    urlcolor=darkish-blue
}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
    \let\ref\Cref

    \title{\bf{Deep Learning Summer School}}
    \date{Summer 2018}
    \author{Michael Noukhovitch}

    \maketitle
    \newpage
    \tableofcontents
    \newpage

    \section{Neural Networks I \ {\small (Hugo Larochelle)}}%
    \label{sec:neural_networks_i}

    \subsection{Basics}%
    \label{sub:basics}

    \subsubsection{Artificial Neuron}%
    \label{ssub:artificial_neuron}
    \begin{description}
        \item[pre-activation] $a(x) = w^T x + b$
        \item[activation function] $h(x) = g(a(x))$
    \end{description}

    \subsubsection{Capacity of Neural Networks}%
    \label{ssub:capacity_of_neural_networks}

    combining two simple linear neurons, can create a more complex non-linear shape \\

    \textbf{universal approximation} a single hidden layer NN can approximate any continuous function arbitrarily well (given enough neurons)

    \subsubsection{Multilayer Neural Network}%
    \label{ssub:multilayer_neural_network}

    we can have $L$ hidden layers, e.g. at layer $k$
    \begin{align*}
        a^{(k)} (x) &= b^{(k)} + W^{(k)}h^{(k-1)} (x) \\
        h^{(k)} (x) &= g(a^{(k)}(x))
    \end{align*}

    \textbf{output layer} ($k = L+1$)
    \begin{equation*}
        h^{(L+1)}(x) = o(a^{(L+1)}(x)) = f(x)
    \end{equation*}

    \subsubsection{Activation Function}%
    \label{ssub:activation_function}

    \begin{description}
        \item[sigmoid] $\frac{1}{1 + \exp(-a)}$ probability of a bernouilli
        \item[tanh] $\frac{\exp(a) - \exp(-a)}{\exp(a) + \exp(-a)}$ squashes between -1 and 1
        \item[relu] $\max(0,a)$ does not saturate
        \item[softmax] multi-class conditional probabilities
    \end{description}

    \subsection{Training Neural Networks}%
    \label{sub:training_neural_networks}

    \subsubsection{Optimization}%
    \label{ssub:optimization}

    learning as an optimization problem
    \begin{description}
        \item[loss function] $l(f(x;\theta), y)$ between output and true label
        \item[regularizer] $\Omega(\theta)$ penalize certain parameters $\theta$
    \end{description}
    use \textbf{Stochastic Gradient Descent}
    \begin{enumerate}
        \item initialize $\theta$
        \item[]for $N$ epochs
        \item[]for each example $x,y$
        \item calculate gradient $\Delta = -\nabla l(\ldots)$
        \item take a step $\theta \gets \theta + \alpha \Delta$
    \end{enumerate}

    \subsubsection{Loss Function}%
    \label{ssub:loss_function}

    maximize the probability of correct class (\textbf{maximum likelihood}) \\
    equivalently \textbf{minimize} the negative log-probability aka \textbf{cross-entropy}

    \begin{equation*}
        l(f(x), y) = - \sum_c 1_(y=c) \ldots
    \end{equation*}

    \subsubsection{Backpropogation}%
    \label{ssub:backpropogation}

    use chain rule to compute gradients

    \begin{enumerate}
        \item compute gradient wrt pre-activation \\
            \begin{equation*}
                \nabla_{a^{(L+1)}(x)} - \log f(x)_y \gets - (e(y) - f(x))
            \end{equation*}
        \item[]for layer $k$ from $L+1$ to $1$
        \item compute gradient of hidden layer parameter
        \item compute gradient of hidden layer below
        \item compute gradient of pre-activation below
    \end{enumerate}

    reversing the flow graph representation gives us backprop for free \ref{sec:automatic_differentiation}


    \subsubsection{Initialization}%
    \label{ssub:initialization}

    \begin{itemize}
        \item biases $\gets 0$
        \item weights $\gets$ random sample
            \begin{itemize}
                \item 0 doesn't work with tanh
                \item same value doesn't work
                \item break symmetry, close to 0
            \end{itemize}
    \end{itemize}

    \subsubsection{Model Selection}%
    \label{ssub:model_selection}

    search for the best hyperparameters with
    \begin{itemize}
        \item \textbf{grid search} search all possible options
        \item \textbf{random search} sample from distribution over hyperparams
        \item bayesian optimization, pbt, \ldots
    \end{itemize}

    use a \textbf{validation set} of examples to choose the best model \\
    stop training with \textbf{early stopping} when validation error is lowest

    \subsubsection{Tricks}%
    \label{ssub:tricks}

    \begin{itemize}
        \item \textbf{normalize} your (real-valued) data
        \item \textbf{decay} your learning rate
        \item update your gradient on a \textbf{batch} of examples
        \item use exponential average of previous gradients, "gaining \textbf{momentum}"
        \item use adaptive learning rates: \textbf{Adagrad}, \textbf{RMSProp}, \textbf{Adam}
    \end{itemize}

    \subsubsection{Gradient Checking}%
    \label{ssub:gradient_checking}

    debug your implementation of fprop/bprop with a finite-difference approximation

    \begin{equation*}
        \frac{df(x)}{dx} \approx \frac{f(x + \epsilon) - f(x - \epsilon)}{2\epsilon}
    \end{equation*}

    \subsubsection{Debug on Small Dataset}%
    \label{ssub:debug_on_small_dataset}

    overfit on a subsubset of your dataset, issues:
    \begin{itemize}
        \item units saturated before first update? initialization, regularization
        \item training error unstable? learning rate scheduling
    \end{itemize}

    \subsection{Regularization}%
    \label{sub:regularization}


    \subsubsection{Unsupervised Pretraining}%
    \label{ssub:unsupervised_pretraining}

    \begin{description}
        \item[unsupervised pretraining] initialize hidden layers by using unsupervised learning to represent the latent structure of data
        \item[fine-tuning] training after initialization to adapt to data
        \item[auto-encoder] feed-forward NN traine to reproduce it's input
    \end{description}

    \subsubsection{Dropout}%
    \label{ssub:dropout}

    \begin{description}
        \item[dropout] remove hidden units stochastically (usually $p = 0.5$) \\

    \end{description}

    \subsubsection{Batch Normalization}%
    \label{ssub:batch_normalization}

    normalize inputs to speed up training
    \begin{itemize}
        \item normalize each unit's pre-activation
        \item subsubtract mean and std deviation, calculated per minibatch
        \item learn a linear transformation of the normalized pre-activation
        \item account for it during backprop
        \item use global mean and std deviation at test time
    \end{itemize}


    \section{Automatic Differentiation \ {\small (David Duvenaud)}}%
    \label{sec:automatic_differentiation}

    \subsection{Basic Autodiff}%
    \label{sub:basic_autodiff}

    \begin{description}
        \item[auto diff] programmatically finding gradients for operations
        \item[forward mode] building the jacobian on, in the order of initial operations (expensive)
        \item[reverse mode] keeping track of jacobian at every step and adding then in reverse
    \end{description}

    \subsubsection{Implementation}%
    \label{ssub:implementation}

    \begin{description}
        \item[static] read and generate source code (Tensorflow)
        \item[dynamic] monitor function exection at runtime (Pytorch, autograd)
    \end{description}

    \begin{enumerate}
        \item trace execution as composition of primitives
        \item define vector-Jacobian product (VJP) operator for each primitive
        \item compose VJPs backwards
    \end{enumerate}


    fun stuff:
    \begin{itemize}
        \item we get higher order autodiff for free \\
            since tape-based dynamic autodiff traces execution, we automatically trace the autodifferentiation itself,
            and just need to follow the execution trace of the previous autodiff
        \item forward mode is actually just a special case of reverse mode
        \item VJPs are as cheap as gradients
    \end{itemize}

    \subsection{Advanced Autodiff}%
    \label{sub:advanced_autodiff}

    \subsubsection{Higher Order Ops}%
    \label{ssub:higher_order_ops}

    higher order gradients are possible by changing vector-Jacobian for vector-Hessian etc \ldots

    \subsubsection{Meta-Optimization}%
    \label{ssub:meta_optimization}

    can optimize through the whole network to learn the learning rate for the training

    \subsubsection{Implicit Function Theorem}%
    \label{ssub:implicit_function_theorem}



    \section{Neural Networks II {\small(Hugo Larochelle)}}%
    \label{sec:neural_networks_ii_small_hugo_larochelle_}

    \subsection{Types}%
    \label{sub:types}

    \begin{description}
        \item[supervised] examples have labels
        \item[unsupervised] examples don't have labels
        \item[semi-supervised] some examples have labels
        \item[multi-task] multiple labels per example
        \item[transfer] multiple labels but test on a specific label
        \item[structured] labels have arbitrary structure
        \item[domain adaptation] training and testing distributions are different
        \item[zero-shot] examples are completely novel
    \end{description}

    \subsection{Intriguing Properties}%
    \label{sub:intriguing_properties}

    \begin{itemize}
        \item there are powerful small changes that create visual adversarial examples
        \item bad local optima are unlikely (for high dimensional loss surface like NNs)
        \item NNs are strangely non-convex
        \item flat minima are better than sharp minima (probably?)
        \item NNs can easily memorize
        \item knowledge can be distilled
        \item catastrophic forgetting is a real issue with SGD
    \end{itemize}

    \section{Intro to Convolutional Neural Networks {\small(Jon Shlens)}}%
    \label{sec:introduction_to_cnns_small_jonathan_shlens_}

    \subsection{Convolutional Neural Networks}%
    \label{sub:convolutional_neural_networks}

    images are different from other data
    \begin{itemize}
        \item fully connected layers would use too many parameters to input an image
        \item convolutions provide translational invariance
    \end{itemize}

    convolutional neural networks
    \begin{itemize}
        \item learn filters that pass over the image
        \item use fewer parameters
        \item use more computations
    \end{itemize}

    \subsection{Modern Developments}%
    \label{sub:modern_developments}

    \begin{itemize}
        \item normalization stabilizes activations and is important for training \\
            Group Normalization (Wu and He 2018)
        \item vanishing gradients motivate deeper, better architectures
        \item architectures transfer across tasks
        \item learned architecture searches find even better models \\
            NAS (Zoph 2016), DARTS (Liu et al 2018)
    \end{itemize}

    \subsection{Understanding CNNs}%
    \label{sub:understanding_cnns}

    how to approach what CNNs understand at each layer?
    \begin{itemize}
        \item find images/pixels that elicit largest activation
        \item reconstruct image from network activations
        \item distort pixels to amplify activations (deep dream)
        \item change lower level activations while maintaining high-level (neural style transfer)
    \end{itemize}

    \section{Deep Computer Vision {\small(Sanja Fidler)}}%
    \label{sec:deep_computer_vision}

    \subsection{Segmentation}%
    \label{sub:segmentation}

    convert classification network to segmentation network
    \begin{enumerate}
        \item pre-training
        \item converting fully connected to fully convolutional layers
    \end{enumerate}
    \hfill \\
    types of segmentation
    \begin{description}
        \item[semantic] assign each pixel a category based on the object it belongs to
        \item[unsupervised] group pixels
        \item[co-segmentation] same objects from different images
        \item[instance] differentiate instances (e.g. car 1, car 2)
        \item[panoptic] instance + semantic
    \end{description}
    \hfill \\
    regularize for segmentation
    \begin{description}
        \item[CRF] conditional random field (unary, pairwise, and optional global term)
        \item[dense CRF] all pixels are connected pairwise
    \end{description}

    \subsection{3D Semantic Segmentation}%
    \label{sub:3d_semantic_segmentation}
    \begin{description}
        \item[fusion] late fusion of CNNs over depth and image
        \item[multi-view] CNN over every viewpoint
        \item[VoxNet] 3D convolution over point cloud
        \item[OctNet] exploit sparsity in 3D representation using octree
        \item[PointNet] represent point clouds directly
        \item[3D Graph NN] point cloud graph
    \end{description}

    \section{Generative Models II {\small(Phil Isola)}}%
    \label{sec:generative_models_ii}

    \subsection{Data Prediction}%
    \label{sub:data_prediction}

    \subsubsection{Conditional Generative Models}%
    \label{ssub:conditional_generative_models}

    challenges
    \begin{enumerate}
        \item output is high-dimensional, structured
        \item uncertainty in mapping, distribution of possibilities
    \end{enumerate}

    two methods
    \begin{enumerate}
        \item model $p(x,)$ and use inference to get $p(x|y)$
        \item directly model $p(x|y)$
    \end{enumerate}

    \textbf{GAN}
    \begin{itemize}
        \item a generator $G$ creates fake images
        \item a discriminator $D$ tries to discern between the fake image and a real image
    \end{itemize}
    \textbf{conditional GAN}
    \begin{itemize}
        \item add class $y$ as extra input to both $G$ and $D$
        \item
    \end{itemize}

    \subsubsection{Structured Prediction}%
    \label{ssub:structured_prediction}

    want to model whole joint
    \begin{itemize}
        \item a GAN with sufficient capacity samples from full joint at equilibrium
        \item needs sufficient capacity and a lot of data
    \end{itemize}

    \textbf{conditional VAE}
    \begin{itemize}
        \item condition on observation $x$
        \item $z$ learns to encode difference between $x$ and $y$
    \end{itemize}

    \subsection{Domain Mapping}%
    \label{sub:domain_mapping}
    \begin{description}
        \item[domain mapping] given two un-paired datasets $x,y$, lets us translate an input $x$ to $y$
        \item[cycleGAN] map from $x \to \hat y \to \hat x$ and minimize reconstruction
            \begin{itemize}
                \item why do we get correct mapping? \textbf{simplicity hypothesis}
                \item minimizing cycle loss in turn minimizes condition entropy
            \end{itemize}
        \item[domain adaption] given classifier in one domain, map it to another domain
    \end{description}

    \subsection{Representation Learning}%
    \label{sub:representation_learning}

    Generative models learn good representations
    \begin{itemize}
        \item generative models must learn to model the data distribution
        \item good models should learn to model the semantics of the data
        \item learned semantics should correspond to the same stuff we care about
    \end{itemize}
    Representation Learning with Contrastive Predictive Coding (van der Oord et al 2018)
    \begin{itemize}
        \item encode sequence with RNN
        \item predict future latent states
        \item modelling new states is now easier since you have predictions about what they should be before you see them
    \end{itemize}

    \subsection{Model-based Intelligence}%
    \label{sub:model_based_intelligence}

    \textbf{Yann Lecun's cake}
    \begin{itemize}
        \item cake: understanding and modelling the world around us well
        \item cherry on top: planning and reasoning using that representation
    \end{itemize}

    deep visual foresight (Finn and Levine 2017)
    \begin{enumerate}
        \item model to predict future frame given action
        \item specify target future frame
        \item select best action for that
    \end{enumerate}
    world models (Ha and Schmidhuber 2018)
    \begin{enumerate}
        \item simulate video game with RNN
        \item train a policy on the simulation (latent space)
        \item policy works on the real game
    \end{enumerate}
    incentivizing exploration in RL with deep predictive models (Stadie et al 2015)
    \begin{enumerate}
        \item model to predict next state given action and previous state
        \item reward agent for visiting "surprising" states
    \end{enumerate}



    \section{Interpretability {\small(Been Kim)}}%
    \label{sec:interpretability_small_been_kim_}

    \subsection{Why Interpretability}%
    \label{sub:introduction}

    there is \textbf{no one-size-fits-all} method for interpretability
    \begin{itemize}
        \item decision trees can't explain the most important factor
        \item rule lists can be too long to be understandable
    \end{itemize}

    the goal is use machine learning more \textbf{responsibly}
    \begin{itemize}
        \item
    \end{itemize}

    the issue is \textbf{underspecification} of what we truly want
    \begin{itemize}
        \item neither more data nor better algorithms can solve it
    \end{itemize}

    \subsection{Interpretability Methods}%
    \label{sub:interpretability_methods}

    before training: \textbf{exploratory data analysis}
    \begin{itemize}
        \item visualize data by features, splits
        \item find examples that are outliers in their class
    \end{itemize}

    during training: building a model
    \begin{itemize}
        \item rule-based methods as a model (e.g. rule lists)
        \item fit a simpler function for each feature
        \item choose representative examples to cluster
        \item train on sparse features
        \item learn a monotonic function (always increasing)
        \item distill your model
    \end{itemize}

    after training: investigate the model
    \begin{itemize}
        \item \textbf{ablation} train without certain features to see their effect
        \item test \textbf{input-feature importance} by sensitivity analysis (e.g. saliency maps)
        \item concept activation vector \textbf{CAV} separates activations of true class and rest
    \end{itemize}

    \subsection{Evaluating Methods}%
    \label{sub:evaluating_methods}

    \begin{itemize}
        \item testing with humans
        \item ground truth experiments
    \end{itemize}

    \section{Theoretical Understanding {\small(Sanjeev Arora)}}%
    \label{sec:theoretical_understanding_sanjeev_arora_}

    \subsection{Optimization}%
    \label{sub:optimization}

    \subsection{Generalization}%
    \label{sub:generalization}

    overparametrization may help optimization

    overfitting is possible

    but we still have excess capacity

    previous ideas
    \begin{itemize}
        \item flat minima is tough to quantify
        \item bounds on true capacity and parameters is elusive
    \end{itemize}

    \textbf{noise stability} networks' higher layers are stable to noise introduced in lower layers
    \begin{itemize}
        \item
    \end{itemize}

    \subsection{Depth}%
    \label{sub:depth}

    does depth help?
    \begin{itemize}
        \item[+] better expressiveness
        \item[-] more difficult optimization
    \end{itemize}
    but overparametrization can actually accelerate training because of SGD

    \subsection{Unsupervised Learning}%
    \label{sub:unsupervised_learning}

    manifold assumption
    \begin{itemize}
        \item learn joint probability $p(X,Z)$
        \item code $Z$ should be a good representation of $X$ on a manifold
    \end{itemize}

    GAN
    \begin{itemize}
        \item generator ``wins'' if objective $\approx 0$, equilibrium
        \item
    \end{itemize}

    mode collapse
    \begin{itemize}
        \item theorem: mode collapse happens if discriminator is too small
        \item for discriminator of size $N$, generator with $N \log N$ images wins
    \end{itemize}

    \subsection{Simpler Methods}%
    \label{sub:simpler_methods}

    we should first understand linear models before approaching deep models
    \begin{itemize}
        \item linear methods for sentence embeddings can be just as effective
    \end{itemize}

    \section{Optimization I {\small(Jimmy Ba)}}%
    \label{sec:optimization_i_small_jimmy_ba_}

    \subsection{Random Search vs Gradient Descent}%
    \label{sub:random_search_vs_gradient_descent}

    learning in neural networks is difficult
    \begin{itemize}
        \item non-convex, many local optima
        \item not smooth, unknown lipschitz constant
        \item millions of trainable weights
    \end{itemize}

    learning with \textbf{random search}
    \begin{enumerate}
        \item perturb weights by random vector $\Delta W \sim N(0,\mu^2I)$
        \item evaluate perturbed averaged loss
        \item add the perturbation weighted by the perturbed loss
        \item[!!] intuitively $d$ samples gives directional gradient,
    \end{enumerate}
    more efficient if we query the gradient directly, \textbf{gradient descent} \\

    \subsection{Better Search Directions}%
    \label{sub:better_search_directions}

    single gradient can be bad on a bad curvature (zig-zagging)
    \begin{itemize}
        \item \textbf{momentum} keeping a running average of gradient updates
        \item momentum with a lookahead trick gives \textbf{nesterov momentum}
        \item stochastically sample a subset, \textbf{batch}, and \textbf{decay learning rate}
    \end{itemize}

    gradient descent needs a constraint for the loss
    \begin{itemize}
        \item \textbf{euclidean} constraints are used normally
        \item \textbf{probability} can be constrained with KL (natural gradient)
    \end{itemize}

    \subsection{``White-Box'' Optimization}%
    \label{sub:_white_box_optimization}

    first order methods can still fail
    \begin{itemize}
        \item second order methods, find good preconditioning matrix that focuses on the weight space that hasn't been explored much
        \item speed up learning for parameters with low variance, \textbf{Adam} and \textbf{Adagrad}
        \item \textbf{distributed learning} with larger batch size
    \end{itemize}

    \textbf{KFAC}, kronecker factored natural gradient
    \begin{itemize}
        \item address scalability in the natural gradient, factorize fischer information
        \item memory efficient and tractable inverse
        \item still computationally expensive, so distribute
    \end{itemize}


    \section{Optimization II {\small(Jorge Nocedal)}}%
    \label{sec:optimization_ii}

    \subsection{Deterministic and Stochastic}%
    \label{sub:deterministic_and_stochastic}

    there is a continuum between two worlds, with similarities
    \begin{itemize}
        \item large-scale non-linear \textbf{deterministic} (optimal trajectory)
        \item \textbf{stochastic} optimization (SGD)
    \end{itemize}

    momentum
    \begin{itemize}
        \item 2D intuition can be deceiving
        \item not convergent on convex functions
        \item similar to CG but that breaks down with noise
    \end{itemize}

    SGD
    \begin{itemize}
        \item deterministic GD has simple proof of convergence in convexity
        \item SGD is more complex, proven to converge with $\alpha \to 0$
    \end{itemize}

    \subsection{First and Second Order}%
    \label{sub:}
    first order methods
    \begin{itemize}
        \item lack of state
        \item not solvable through universal formula
        \item suffer from ill-conditioning
    \end{itemize}

    second order information
    \begin{itemize}
        \item inexact newton method with hessian sub-sampling
        \item natural gradient methods
        \item quasi-newton with progressive sampling
    \end{itemize}

    \subsection{SGD}%
    \label{sub:sgd}

    tricks to converge
    \begin{itemize}
        \item changing batch size instead of step size
        \item robust optimization (similar to adversarial learning)
        \item progressive sampling batch size, and learning to sample
    \end{itemize}

    instead of scaling step size, focus on step direction $\to $ \textbf{newton's method}
    \begin{itemize}
        \item \textbf{quasi-newton} subsamples in the case when matrix is too big to invert
        \item newton moves in direction of smallest eigenvalue, so approximate \textbf{inexact newton} approximates the smallest
        \item non-convex case can be solved by following \textbf{newton-CG} until negative curvature, then following it
    \end{itemize}

    \section{Recurrent Neural Networks}%
    \label{sec:recurrent_neural_networks}

    \subsubsection{Reccurent Neural Networks}%
    \label{ssub:reccurent_neural_networks}

    \subsubsection{Generative RNNs}%
    \label{ssub:generative_rnns}


    \subsubsection{Conditional RNNs}%
    \label{ssub:conditional_rnns}

    \begin{itemize}
        \item sequence to vector
        \item vector to sequence
        \item sequence to sequence
    \end{itemize}

    \textbf{teacher forcing} during training, previous $y$ is true $y$ taken from training data
    \begin{itemize}
        \item at test time, prev $y$ is generated
        \item mismatch
    \end{itemize}
    \textbf{scheduled sampling} stochastically choose to either generate or take $y$
    \textbf{professor forcing} using a GAN to sample trajectories?


    \subsection{Deeper RNNs}%
    \label{sub:deeper_rnns}

    \begin{description}
        \item[stacking] stacking hidden layers
        \item[mixing hidden, input, output] hid to out, hid to hid, in to hid
        \item[skip connections] mixing, with skips
    \end{description}

    \subsubsection{Architechtures}%
    \label{ssub:architechtures}

    dimensional
    \begin{description}
        \item[recursive] tree-structured recursive computation
        \item[multi-directional] 2D graph where input comes from two separate dimensions
        \item[bidirectional] concatenation of forward and backward RNNs hidden states
    \end{description}

    modifications
    \begin{description}
        \item[mutliplicative] replace addition by dot product in hidden
        \item[multi-scale] different time scale RNNs
    \end{description}

    \subsection{Long Term Dependencies}%
    \label{sub:long_term_dependencies}

    \subsubsection{The Problem}%
    \label{ssub:the_problem}

    a simple neuron can store 1 bit as a dynamical system
    \begin{itemize}
        \item basins of attraction
        \item gradient must be high at boundary of basins
    \end{itemize}
    spectral radius determines stability
    \begin{itemize}
        \item if > 1 then noise can kick neuron out of state
        \item if < 1 it is stable
    \end{itemize}
    gradient calculation requires multiplying each neuron's matrix
    \begin{itemize}
        \item if spectral norms are < 1 $\to$ total product goes to 0
    \end{itemize}

    gradient that is propogated will therefore either increase (if above 1) or decrease (if below 1) causing it to either \textbf{explode} or \textbf{vanish}


    \subsubsection{Solutions}%
    \label{ssub:solutions}

    delays and hierarchies
    \begin{itemize}
        \item learned time scales / hierarchies (Chung et al 2016)
        \item hierarchical multiscale RNNs (Chung et al 2017)
    \end{itemize}

    gating
    \begin{itemize}
        \item self-loop for gradients, learned gating
        \item eigenvalue of Jacobian only slightly less than 1
        \item LSTM (Hochreiter and Schmidhuber 1997)
        \item GRU (Cho et al 2014)
    \end{itemize}

    attention
    \begin{itemize}
        \item learn to weigh your whole input vector based on current state
        \item soft content-based attention (Bahdanau et al 2014)
    \end{itemize}


    \subsubsection{Attention}%
    \label{ssub:attention}

    attention networks
    \begin{itemize}
        \item graph attention networks (Velickovic 2018)
        \item attention for memory, NTM (Bahadanau et al 2014)
        \item pointing unknown words (Gulcehre 2016)
    \end{itemize}

    self-attention (Google 2017)
    \begin{itemize}
        \item encode location, transform location based on attention from others
        \item parallalelized
        \item self-attentive backtracking (Ke et al 2018)
    \end{itemize}

    \subsubsection{Consciousness Prior}%
    \label{ssub:consciousness_prior}

    \begin{itemize}
        \item we reason in latent space
        \item our representations have few, low-dimensional variables
        \item constrain by selecting relevant abstract concepts
    \end{itemize}

    two levels of representation: \textbf{consciousness prior}
    \begin{itemize}
        \item high-dependsional abstract representation
        \item low-dimensional conscious thought
        \item attention mechanism over both to create conscious thought
        \item objective function in abstract space, mutual information between past and future
    \end{itemize}

    \section{Language Understanding {\small(Graham Neubig)}}%
    \label{sec:language_understanding_small_graham_neubig_}

    \subsection{Overview}%
    \label{sub:overview}

    \begin{itemize}
        \item language modelling
        \item text classification
        \item sequence transduction
        \item structurual analysis
        \item semantic analysis
    \end{itemize}

    \subsection{Sentence Classification}%
    \label{sub:sentence_classification}

    bag of words
    \begin{itemize}
        \item sum of word's value + softmax
        \item can't handle negations, double-negatives
    \end{itemize}

    deep continuous bag of words
    \begin{itemize}
        \item cbow + tanh activations
        \item effective but still can't handle sequential dependencies
    \end{itemize}

    bag of ngrams, CNNs
    \begin{itemize}
        \item ngrams: sliding window over sentence
        \item soft bag of ngrams + pooling: time delay NN (Waibel et al 1989) $\approx$ 1D conv
        \item great for short-distance feature extractions
        \item fail on long-distance dependencies
    \end{itemize}

    RNN
    \begin{itemize}
        \item recurrence allows for "remembering"
        \item long term and sequential dependencies
        \item credit assignment is difficult
    \end{itemize}

    count-based models
    \begin{itemize}
        \item counting frequency and dividing
    \end{itemize}

    \subsection{Language Models}%
    \label{sub:stuff}

    word embedddings
    \begin{itemize}
        \item create a space for words that semantically represents them
        \item sharing strength and reducing parameter space
    \end{itemize}

    RNN LMs
    \begin{itemize}
        \item RNNs using word embeddings
        \item long terms dependencies and also smaller parameters space
        \item effective and standard method
    \end{itemize}

    evaluation
    \begin{itemize}
        \item log-likelihood
        \item per-word log-likelihood
        \item per-word cross entropy ($\log 2$)
        \item perplexity ($e^{\text{LL}}$)
    \end{itemize}

    \subsubsection{Conditioned Language Models}%
    \label{ssub:conditioned_language_models}

    generate text according to specification
    \begin{itemize}
        \item condition RNN through hidden state
    \end{itemize}

    \subsection{Generation Problem}%
    \label{sub:generation_problem}

    \begin{description}
        \item[sampling] generate a random sentence
        \item[ancestral sampling] generate words one-by-one
    \end{description}

    \begin{itemize}
        \item \textbf{greedy search} pick word with highest prob
        \item \textbf{beam search} keep multiple hypotheses
    \end{itemize}

    \begin{description}
        \item[argmax] generate sentence with highest prob, e.g. sentence translation
    \end{description}
    \begin{itemize}
        \item \textbf{attention} keep vector of context
        \item \textbf{self-attention}
    \end{itemize}

    \subsection{Parsing and Tagging}%
    \label{sub:parsing}

    linearized tree + seq2seq
    \begin{itemize}
        \item make your tree a string and ``translate''
        \item simple but doesn't exploit structure
    \end{itemize}

    bi-LSTM CRF
    \begin{itemize}
        \item CRF layer ensure consistency between tags
        \item training and test with dynamic programming
    \end{itemize}

    stack LSTMs (Dyer et al 2015)
    \begin{itemize}
        \item 3 LSTMS: over words and over compositional representation
        \item gives better structural and semantic understanding, exploit structure
    \end{itemize}

    \subsection{Small Data}%
    \label{sub:small_data}

    pre-training on another task
    \begin{itemize}
        \item word embeddings (Fasttext)
        \item sentence representation (Dai et al 2015)
        \item contextual word embeddings (ELMo)
    \end{itemize}

    multi-task learning
    \begin{itemize}
        \item jointly training on your task and another
    \end{itemize}

    cross-domain
    \begin{itemize}
        \item learn over many languages simultaneously to help translating other
        \item fine-tuning on your specific language
        \item adding language tags
    \end{itemize}

    design models that scale, similar to our own linguistic understanding

    \subsection{Semantics}%
    \label{sub:semantics}

    \subsubsection{Semantic Parsing}%
    \label{ssub:semantic_parsing}

    executable representations
    \begin{itemize}
        \item parsing to SQL queries
        \item parsing to commands, compositional trees
        \item parsing to source code
    \end{itemize}

    semantic parsing with syntax
    \begin{itemize}
        \item express as abstract syntax tree (AST)
        \item works better than just seq2seq
    \end{itemize}


    \subsubsection{Machine Reading}%
    \label{ssub:machine_reading}

    multiple-choice question tasks
    \begin{itemize}
        \item MCTest 500 passages, 2k questions
        \item RACE 28k passages, 100k questions
    \end{itemize}

    span selection
    \begin{itemize}
        \item SQuAD 500 passages, 100k questions
        \item triviaQA
    \end{itemize}

    necessary
    \begin{itemize}
        \item extract only salient parts
        \item do abstract reasoning on language
    \end{itemize}

    bidirectional language flow (Seo et al 2017)
    \begin{itemize}
        \item calculate doc2ctxt, ctxt2doc
        \item concatenate representations and extract
    \end{itemize}

    memory networks (Sukhbaatar et al 2015)
    \begin{itemize}
        \item multiple layers of attention to save/read info
        \item does "reasoning"
    \end{itemize}

    \subsection{Challenges}%
    \label{sub:challenges}

    \begin{itemize}
        \item scaling down to scarce data
        \item robust solutions in big data scenarios
        \item deeper reasoning
        \item interpretability
    \end{itemize}


    \section{Multi-modal Learning {\small (Jamie Kiros)}}%
    \label{sec:multi_modal_learning_small_jamie_kiros_}

    \begin{itemize}
        \item encoding
        \item decoding
        \item \ldots
    \end{itemize}

    \subsection{Building Blocks}%
    \label{sub:building_blocks}

    \begin{description}
        \item[matching] learning a joint embedding space for retrieval
        \item[fusion] combining representations of modalities
            \begin{itemize}
                \item simple
                \item gating
                \item bilinear pooling
            \end{itemize}
        \item[FiLM] condition one modality on another
            \begin{itemize}
                \item VQA (Perez et al 2017)
                \item RL with instructions (Chaplot et al 2017)
            \end{itemize}
        \item[translation] from one modality to another
    \end{description}

    \subsection{Research}%
    \label{sub:research}

    \begin{description}
        \item[grounding] learning one modality given correspondance in another
        \item[contextualization] harness large corpus to learn contextualized word embeddings (Peters et al 2018)
            \begin{itemize}
                \item winograd schemas
            \end{itemize}
        \item[multi-apt representations] flexible modulation across contexts/tasks
            \begin{itemize}
                \item InferLite (Kiros and Chan 2018) multi-apt sentence embeddings
            \end{itemize}
        \item[relevance realization] zoning in on what's relevant
            \begin{itemize}
                \item dynamic representation, changing with relevance
                \item aligning books and movies (Kiros and Zhu 2015)
            \end{itemize}
        \item[specificity] certain images have more variability in their possible descriptions
            \begin{itemize}
                \item increase specificity through grounding, GuessWhat?! (De Vries et al 2017)
                \item Hierarchical Neural Story Generation (Fan et al 2018)
            \end{itemize}
    \end{description}

    \section{Generative Models for Music {\small(Sageev Oore)}}%
    \label{sec:generative_models_for_music_small_sageev_oore_}

    \subsection{Music}%
    \label{sub:music}

    \subsubsection{Music Background}%
    \label{ssub:music_background}

    \begin{description}
        \item[pitch] perceptual concept
        \item[interval] distance between pitches, not easily relative
        \item[consonance] ``niceness'' not always good, not invariant
        \item[dynamics] you can ``weight'' notes, changes overall sound
    \end{description}

    it is difficult to evaluate a melody, a good composer might make any melody work

    \subsubsection{Representations}%
    \label{ssub:representations}

    \begin{description}
        \item[MIDI] sequential description of music data
            \begin{itemize}
                \item good for pitch
                \item not good for timbre
            \end{itemize}
        \item[score] what notes to play, interpreted by performer
        \item[spectrogram] frequencies and their intensity
    \end{description}

    we can consider MIDI / score a sort of \textit{language}

    \subsection{Modelling}%
    \label{sub:modelling}

    \subsubsection{Score}%
    \label{ssub:score}

    sequence tutor (jacques et al 2017)
    \begin{itemize}
        \item RNN that has to follow rules made by a Q-network
    \end{itemize}

    musical dialogue (Bretan et al 2017)
    \begin{itemize}
        \item call and response music
        \item music embeddings with an auto-encoder
        \item clustering + unit selection to create response
    \end{itemize}

    "flat" note VAE
    \begin{itemize}
        \item hierarchical decoder with biLSTM encoder
    \end{itemize}

    \subsubsection{Performance}%
    \label{ssub:performance}

    performance RNN (Simon and Oore 2017)
    \begin{itemize}
        \item train on single-instrument human performance
        \item generate directly in the performance space
    \end{itemize}


    relative self-attention for music (Huang et al 2018)
    \begin{itemize}
        \item using the transformer for music generation
    \end{itemize}

    \subsubsection{Audio}%
    \label{ssub:audio}

    musical timbre transfer (Huang et al 2018)
    \begin{itemize}
        \item cycleGAN to transform waveform
        \item transfer flute to violin
    \end{itemize}


\end{document}
