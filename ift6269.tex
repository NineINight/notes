\documentclass[]{article}
\usepackage{etex}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{color}
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage[lined]{algorithm2e}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage[pdftex,
  pdfauthor={Michael Noukhovitch},
  pdftitle={IFT6269: Probabilistic Graphical Models},
  pdfsubject={Lecture notes from IFT6269 at Univerite de Montreal},
  pdfproducer={LaTeX},
  pdfcreator={pdflatex}]{hyperref}

\usepackage{cleveref}
\usepackage{enumitem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem{ex}{Example}[section]
\newtheorem*{theorem}{Theorem}

\setlength{\marginparwidth}{1.5in}
\setlength{\algomargin}{0.75em}

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}

\definecolor{darkish-blue}{RGB}{25,103,185}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=darkish-blue,
    filecolor=darkish-blue,
    linkcolor=darkish-blue,
    urlcolor=darkish-blue
}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\1}[1]{\mathds{1}_#1}
\newcommand{\distas}[1]{\mathbin{\overset{#1}{\kern\z\sim}}}%
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand{\distras}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
	\let\ref\Cref

	\title{\bf{IFT6269: Probabilistic Graphical Models}}
	\date{Fall 2017, Universite de Montreal \\ \center Notes written from Simone Lacoste-Julien's lectures.}
	\author{Michael Noukhovitch}

	\maketitle
	\newpage
	\tableofcontents
	\newpage
	
	\section{Probability Review}
	\label{sec:probability_review}

	we need probability to model \textbf{uncertainty}
	\begin{itemize}
		\item intrinsic (quantum mechanics)
		\item incomplete information (dice)
		\item incomplete modelling ("most birds can fly")
	\end{itemize}

	\subsection{Notation}
	\label{sub:notation}
	
	\begin{description}
		\item[sample space] $\Omega$
		\item[realization] $x_1 \in \Omega$
		\item[random variable] a measurable mapping $X: \Omega \rightarrow \mathbb{R}$ 
		\item[indicator function] $\1{A}(w) = \begin{cases}
				1, &\text{ if } w \in A \\
				0, &\text{ else}	
			\end{cases}$
		\item[probability distribution] a mapping $P: 2^\Omega \rightarrow [0,1]$
		\item[set of events] $E = $ set of all subsets of $\Omega$
		\item[event] $\{ X = x_1\}$ represents both 
			\begin{itemize}
				\item the event $\{ x \} \in \Omega_x$ 
				\item the event $\{ w \in \Omega : X(w) = x_1 \} \in E$
			\end{itemize}  
		\item[joint distribution] a random vector $P_{x,y}\{X=x,Y=y\}$ 
		\item[marginal distribution] distribution on the components of the random vector  \\ 
			$P\{X = x \} = \sum_{y = \Omega_y} P\{X = x, Y = y \}$  \\ 
	\end{description}


	\subsection{Kologomorov Axioms}
	\label{sub:kologomorov_axioms}

	\begin{enumerate}
		\item $P(E_i) \geq 0, \forall E_i \in E$
		\item $P(\Omega) = 1$
		\item $P(\cup_{i=1}^{\infty} E_i) = \sum_{i=1}^\infty P(E_i)$ when $E_i$s are disjoint
	\end{enumerate}

	\subsection{Random Variable Basics}
	\label{sub:random_variable_basics}

	\begin{description}
		\item[probability mass function] $P_X(x) = P \{X = x\}, x \in \Omega_x$
		\item[cumulative distribtion function] $F_X(x) = P \{ X \leq x \}$
			\begin{itemize}
				\item non-decreasing
				\item $ \lim_{x \to - \infty} F_X(x) = 0$
				\item $ \lim_{x \to \infty} F_X(x) = 1$
			\end{itemize}
		\item[probability density function] function $p(x)$  s.t. $F_X(x) = \int p(x)dx$
		\item[discrete variable] $\Omega_x$ is countable, definted by its PMF
		\item[continuous] $\Omega_x$ is uncountable, defined by its PDF
	\end{description}

	\subsection{Other Probability Review}
	\label{sub:other_probability_review}
	\begin{description}
		\item[expectation/mean] \begin{align*}
				E[X] &= \sum_{x \in \Omega_x} x p(x) \\
				 	 &= \int_{\Omega} x p(x) d(x)
			\end{align*}
		\item[variance] $Var[X] = E[(X - E[X])^2]$
		\item[independence] $X \independent Y$ if $p(x,y) = p(x)p(y)$
		\item[conditional] $P(A|B) = \frac{P(A \cap B)}{P(B)}$
	\end{description}
	
	\subsection{Rules}
	\label{sub:rules}
	
	\begin{description}
		\item[bayes rule] $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
		\item[product rule] $P(A,B) = P(A|B)P(B)$
		\item[conditional independence] $X \independent Y | Z \iff p(x,y|z) = p(x|z)p(y|z)$
	\end{description}	

	\section{Parametric Models}
	\label{sec:parametric_models}

	\subsubsection{Bernoilli}
	\label{ssub:bernoilli}
	A coin flip with probability $\theta, X \sim Bern(\theta)$
	\begin{itemize}
		\item $p(x=1|\theta) = \theta$
		\item $\Theta = [0,1]$
		\item $\Omega_x = \{0,1\}$
		\item $E[X] = \theta$
		\item $Var[X] = \theta(1-\theta)$
	\end{itemize}

	\subsubsection{Binomial}
	\label{ssub:binomial}
	$N$ independent coin flips, $X \sim Bin(n, \theta)$ \\
	\begin{itemize}
		\item let $X_i \distras{iid} Bern(\theta)$, then $X = \sum^n X_i$
		\item $p(x;\theta) = {n \choose x} \theta^x (1-\theta)^{n-x}$
		\item $E[X] = n \theta$
		\item $Var[X] = n \theta (1 - \theta)$
	\end{itemize}
	
	\subsubsection{Other Distributions}
	\label{ssub:other_distributions}
	\begin{itemize}
		\item Poisson $\Omega_x = \{0, 1, \ldots \} = \mathbb{N}$
		\item Gaussian $N(\mu, \sigma^2), \Omega_x = \mathbb{R}$
		\item Gamma $\Gamma(\alpha, \beta), \Omega_x = \mathbb{R}_+$
	\end{itemize}
	

	\section{Probability}
	\label{sec:probability}
	
	\subsection{Maximum Likelihood Estimator}
	\label{sub:maximum_likelihood_estimator}

	Maximize $p(x | \theta)$ for binomial where $p(x | \theta) = \binom{n}{x} + x^$ \\
	We use log likelihood instead, because if $a < b$ then $\log a < \log b$
	\begin{equation*}
		\log \binom{n}{x} + n \log x + (n-k) \log (1-x)
	\end{equation*}
	so $f'(\theta) = 0, f''(\theta) = 0$ is necessary condition for local max
\end{document}
