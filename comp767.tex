\documentclass[]{article}
\usepackage{etex}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{color}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage[lined]{algorithm2e}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage[pdftex,
  pdfauthor={Michael Noukhovitch},
  pdftitle={COMP767},
  pdfsubject={Lecture notes from Doina Precup},
  pdfproducer={LaTeX},
  pdfcreator={pdflatex}]{hyperref}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{enumitem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  language=C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem{ex}{Example}[section]
\newtheorem*{theorem}{Theorem}

\setlength{\marginparwidth}{1.5in}
\setlength{\algomargin}{0.75em}

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}

\definecolor{darkish-blue}{RGB}{25,103,185}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=darkish-blue,
    filecolor=darkish-blue,
    linkcolor=darkish-blue,
    urlcolor=darkish-blue
}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Lagr}{\mathcal{L}}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\graphicspath{{./comp767/}}

\begin{document}
\let\ref\Cref

\title{\bf{COMP767: Reinforcement Learning}}
\date{Winter 2018, \\ \center Notes written from Doina Precup's lectures.}
\author{Michael Noukhovitch}

\maketitle
\newpage
\tableofcontents
\newpage

\section{Introduction}
\label{sec:introduction}

\subsection{Definitions}
\label{sub:definitions}
Reinforcement learning is:
\begin{description}
    \item[agent-oriented learning] learning by interacting with an environment
    \item[trial and error] only given delayed evaluative feedback
    \item[science of the mind] one which is neither natural science nor applied technology
\end{description}

Framework:
\begin{enumerate}
    \item agent percieves the \textbf{state} of the environment
    \item based on the state, it chooses an \textbf{action}
    \item the action gives the agent a \textbf{reward}
    \item a \textbf{policy} aims to maximize the agent's \textbf{long term expected reward}
\end{enumerate}


\subsection{Key Factors of RL}
\label{sub:key_factors_of_rl}
\begin{itemize}
    \item trial and error search
    \item environment is stochastic
    \item reward may be delayed
    \item balancing exploration and exploitation
\end{itemize}

\subsection{Classical Challenges}
\label{sub:classical_challenges}
\begin{itemize}
    \item reward
    \item information is sequential
    \item delayed consequences
    \item balancing exploration/exploitation
    \item non-stationarity
    \item fleeting nature of time and online data
\end{itemize}


\section{Bandit}
\label{sec:bandit}

\subsection{Definition}
\label{sub:definition}

\textbf{One-armed bandit} Simplest RL problem
\begin{itemize}
    \item pull the lever
    \item get some reward
    \item choose the best lever!
\end{itemize}

\textbf{k-armed bandit} extends to $k$ arms
\begin{itemize}
    \item at every time step $t$, choose an action $A_t$ from $k$ possibilties
    \item recieve a reward $R_t$ dependent only on the action taken (i.i.d)
    \item $q_*(a) = \E [R_t | A_t = a], \medskip \forall a \in {1, \ldots k }$
\end{itemize}

\subsection{Action Selection}
\begin{description}
    \item[greedy] the action with the current highest expected value (best one so far)
    \item[exploitation] choosing the greedy action
    \item[exploration] choosing not the greedy action
    \item[$\varepsilon$-greedy] balance explore/exploit by choosing exploration (random) with probability $\varepsilon$
\end{description}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\linewidth]{epsilon_10arm.png}
    \caption{$\epsilon$-greedy methods on 10-arm bandit}
    \label{fig:}
\end{figure}

\subsection{Learning Rules}
\label{sub:learning_rules}

Learn the best policy by learning the reward for an action

\subsubsection{Averaging}
\label{ssub:averaging}

For a single action, update the new estimate based on old estimate and step size ($\alpha$), with all actions being equal
\begin{equation*}
    Q_{n+1} = Q_n + \alpha (R_n - Q_n)
\end{equation*}

\subsubsection{Recency-Weighted Average}
\label{ssub:recency_weighted_average}

\begin{description}
    \item[stationary] if the true action values DO NOT change over time
\end{description}

if our bandit is non-stationary, then we need to put more weight on recent samples 
\begin{equation*}
    Q_{n+1} = (1 - \alpha)^n Q_1 + \sum_{i=1}^n \alpha(1 - \alpha)^{n - i} R_i
\end{equation*}

\subsubsection{Optimistic}
\label{ssub:optimistic}
Previously we assumed $Q_1(a) = 0$, but we can start optimistically (e.g. $Q_1(a) = 5$) to encourage early exploration

\subsubsection{Upper Confidence Bound}
\label{ssub:upper_confidence_bound}
Reduce exploration over time after starting confident
\begin{itemize}
    \item estimate upper bound on true action values
    \item select the action with the largest upper bound
\end{itemize}

\begin{equation*}
    A_t = \argmax_a [Q_t(a) + c \sqrt{\frac{\log t}{N_t(a)}}]
\end{equation*}

\subsubsection{Gradient-Bandit Algorithms}
\label{ssub:gradient_bandit_algorithms}

Don't need to learn specific rewards, just learn the \textbf{preference} $H_t(a)$, and try and make the probability of choosing an action $\pi_t(a)$ be proportional to it.

\begin{eqnarray*}
    \pi_t(a) &\propto e^{H_t(a)} \\
             &= \frac{e^{H_t(a)}}{\sum_b e^{H_t(b)}}
\end{eqnarray*}

if the reward for an action is better than average, increase its preference
\begin{eqnarray*}
    H_{t+1} &= H_t(a) + \alpha(R_t - \bar R_t)(1_{a = A_t} - \pi_t(a)) \\
\end{eqnarray*}
where $\bar R_t = $ average $R_i$

\subsubsection{Associative Search}
\label{ssub:associative_search}

\begin{description}
    \item[associative] a task where the situation/state of the agent changes the reward for an action
    \item[contextual bandit] not just trial-and-error search, but also association between state and action values 
    \item[full reinforcement learning] trial-and-error search, association between state and action, and actions affecting the next state of the agent
\end{description}


\subsection{Evaluations}
\label{sub:evaluations}
\begin{description}
    \item[regret] the difference between best option and the one we chose $\max_a q_*(a) - q_t(a)$
    \item[expected total regret] $\E[\sum_t $ regret$_t]$ (optimal for UCB, Thomson sampling)
    \item[best response] regret for $T$ experimental trials after policy is fixed
\end{description}

\subsection{Conclusions}
\label{sub:conclusions}
\begin{itemize}
    \item simple methods that can be built on
    \item learn from feedback
    \item appear to have a goal
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\linewidth]{bandit_comparison.png}
    \caption{bandit algorithm comparison}
    \label{fig:bandit-comparison}
\end{figure}

\section{Markov Decision Processes}
\label{sec:markov_decision_processes}

\subsection{Markov}
\label{sub:markov}

\begin{description}
    \item[markov property] future independent of past given present
    \item[markov chain] memoryless random process with states $S$ and transition probabilities $P$, $<S,P>$

    \item[markov reward process] markov chain with values, rewards for states $R$, discount factor $\gamma$
    \item[return] sum of discounted rewards $G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} \ldots$
    \item[value function] long-term value of state $s$, $v(s) = E[G_t | S_t = s]$
\end{description}

\subsection{Returns and Values}
\label{sub:returns_and_values}

\begin{description}
\end{description}




\subsection{Policy Iteration}
\label{sec:policy_iteration}

\subsection{Asynchronous DP}
\label{sub:asynchronous_dp}

\begin{itemize}
    \item choose
\end{itemize}

\section{Monte Carlo Methods}
\label{sec:monte_carlo_methods}

\subsection{MC Policy Evaluation}
\label{sub:mc_policy_evaluation}

learn $v_\pi(s)$ given some number of episodes under $\pi$ which contain $s$, using the average of the rewards after visiting $s$
\begin{description}
    \item[every visit MC] average returns for every visit to $s$
    \item[first visit MC] average returns for only the first visit to $s$ (in an episode)
\end{description}

\subsection{On-Policy MC Control}
\label{sub:on_policy_mc_control}

\begin{description}
    \item[on policy] learn about policy currently executing
    \item[boltzmann exploration] choose with probability $\exp (- \frac{P(s)}{T} )$ where $T$ is a temperature.  \\
        Over time, decrease from high $T$ (equiprobable) to low $T$ (biased towards best action)
\end{description}

\subsection{Advantages}
\label{sub:advantages}
Several advantages over DP
\begin{itemize}
    \item can learn directly from interaction with environment
    \item no need for full models
    \item no need to learn about all states
    \item less harmed by violating Markov property
\end{itemize}



\end{document}
