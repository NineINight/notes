\documentclass[]{article}
\usepackage{etex}
\usepackage[margin = 1.5in]{geometry}
\setlength{\parindent}{0in}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{listings}
\usepackage{color}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{pgfplots}
\usepackage{qtree}
\usepackage{xytree}
\usepackage[lined]{algorithm2e}
\usepackage{float}
\usepackage[T1]{fontenc}
\usepackage{ae,aecompl}
\usepackage[pdftex,
  pdfauthor={Michael Noukhovitch},
  pdftitle={EDA040: Concurrent Programming},
  pdfsubject={Lecture notes from EDA040 at the Lund University},
  pdfproducer={LaTeX},
  pdfcreator={pdflatex}]{hyperref}

\usepackage{cleveref}
\usepackage{enumitem}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4
}

\theoremstyle{definition}
\newtheorem*{defn}{Definition}
\newtheorem{ex}{Example}[section]
\newtheorem*{theorem}{Theorem}

\setlength{\marginparwidth}{1.5in}
\setlength{\algomargin}{0.75em}

\DeclarePairedDelimiter{\set}{\lbrace}{\rbrace}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\definecolor{darkish-blue}{RGB}{25,103,185}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=darkish-blue,
    filecolor=darkish-blue,
    linkcolor=darkish-blue,
    urlcolor=darkish-blue
}
\newcommand{\lecture}[1]{\marginpar{{\footnotesize $\leftarrow$ \underline{#1}}}}

\makeatletter
\def\blfootnote{\gdef\@thefnmark{}\@footnotetext}
\makeatother

\begin{document}
	\let\ref\Cref

	\title{\bf{EDA040: Concurrent Programming}}
	\date{Fall 2015, Lund University\\ \center Notes written from Klas Nilsson's lectures.}
	\author{Michael Noukhovitch}

	\maketitle
	\newpage
	\tableofcontents
	\newpage

	\section{Introduction}
	\subsection{Concurrency}
	\begin{description}
		\item[activity] entity performing actions
		\item[process] entity performing instructions with own resources
		\item[job] sequential instructions to be performed by an activity
		\item[task] a set of jobs being performed by some process
		\item[thread] sequential activity performing instructions
		\item[execution thread] the thread itself accessed via the \lstinline|Thread| interface
	\end{description}

	\subsection{Mutual Exclusion}
	requirements:
	\begin{itemize}
		\item mutual exclusion
		\item no deadlock
		\item no starvation
		\item efficiency
	\end{itemize}
	\section{Semaphores}
	\textbf{semaphore} simple counting interface for concurrency
	\subsection{Mutex}
	used to lock and unlock critical sections
	\begin{lstlisting}[language=Java]
MutexSem mutex;
mutex.take()
// critical section
mutex.give()
	\end{lstlisting}
	\subsection{Signaling}
	calls used to block or unblock a thread
	\begin{lstlisting}[language=Java]
CountingSem mutex = new CountingSem();
// thread A
*
mutex.take() // block this thread
*
// thread B
*
mutex.give() // unblock thread A
*
	\end{lstlisting}

	\subsection{Other Types}
	\begin{itemize}
		\item blocked-set: arbitrary thread \lstinline|take|s 
		\item blocked-queue: \lstinline|take| in FIFO order
		\item blocked-priority: highest priority \lstinline|take|
		\item binary semaphore: efficient mutex implementation in RTOS
		\item multi-step semaphore: reserve several resources at once
	\end{itemize}

	\section{Monitors}
	\subsection{Introduction}
	As opposed to using \lstinline|take/give| throughout the program, we instead can limit our mutual exlcusions to specific function. \\
	\textbf{Monitor}: interface for mutually exclusive access to a function, in java using \lstinline|synchronized|
	\begin{itemize}
		\item \lstinline|wait| stateless wait for signal
		\item \lstinline|notify| notify first (or highest prio) waiting task
		\item \lstinline|notifyAll| notify all waiting task
	\end{itemize}
	\subsection{Rules}
	\begin{itemize}
		\item don't mix a thread and monitor
		\item all public methods should be synchronized
		\item wrap thread-unsafe classes by monitor
		\item don't use (spread-out) synchronized blocks
	\end{itemize}

	\section{Deadlock}
	\subsection{Introduction}
	\begin{description}
		\item[deadlock] a circular chain of tasks trying to allocate resources
		\item[starvation] when a task is never prioritized to execute
		\item[livelock] a running circular chain that is unable to allocate resources
	\end{description}

	Deadlock \textbf{detection} is not feasible as a resolution for real-time applications so we will looks at \textbf{prevention} which deals with eliminating one of the conditions for deadlock:
	\begin{itemize}
		\item mutual exclusion
		\item hold and wait
		\item no preemption
		\item circular wait
	\end{itemize}

	\subsection{Dining Philosophers}
	Five dining philosophers with five forks between them but each needs two to eat, proves to be a deadlock-able situation if they circularly pick up one fork. We can solve it with: 
	\begin{itemize}
		\item One left-handed philosopher that picks up left fork first (not circular)
		\item Only allowing four philosophers into the room at a time (monitor)
		\item Philosophers picking up both forks or neither (using a \lstinline|Multistep Sem|, starvation possible)
	\end{itemize}

	\section{Message-based Synchronization}
	\subsection{Mailboxes}
	Message-based communication is useful for:
	\begin{itemize}
		\item producer-consumer relations
		\item signaling (one thread never waits)
		\item information transfer (in data of message)
		\item buffering
		\item distributed concurrency
		\item encapsulation (concurrent object properties)
	\end{itemize}

	\subsection{Unbounded mailbox}
	Use copy-on-send to create limitless mailboxes
	\begin{itemize}
		\item[+] flexible code
		\item[+] no need to assume shared memory
		\item[+] thread safety, message not accessible by sender
		\item[-] can run out of memory, increased memory use
		\item[-] unpractical when immediate response is required
		\item[-] recycling via message pools is difficult
	\end{itemize}

	\subsection{Implementation}
	We will use \lstinline|java.util.EventObject| as our message and \lstinline|RTEvent| for async communication because it includes a timestamp. As such we will use \lstinline|RTEventBuffer| as our circular mailbox
	\section{Scheduling}
	\subsection{Timing}
	We want to perform something periodically:
	\begin{description}
		\item[CyclicThread]	no specific period
		\item[PeriodicThread] specific periodic
		\item[SporadicThread] minimum period
	\end{description}

	We are also interested in maintaining strict deadlines, we define some terms:
	\begin{description}
		\item[latency] time between release time and start time
		\item[execution time] time from start until finish
		\item[response time] latency + execution time
	\end{description}

	\subsection{Metrics}
	Use \textbf{WCET} (worst case execution time) or \textbf{R} (worst case response time)
	\begin{itemize}
		\item high priority
			\begin{itemize}
				\item max latency: time to context switch
				\item max execution: maximum blocking time
			\end{itemize}
		\item low priority
			\begin{itemize}
				\item max latency: WCET for all higher and equal priority threads
				\item max execution: WCET for all higher priorities (preemption) and max blocking time
			\end{itemize}
	\end{itemize}

	\subsection{Static Scheduling}
	Time is divided into short slots and all activities are scheduled in advance, with cyclical execution
	\begin{itemize}
		\item[+] guaranteed scheduling
		\item[+] easy to calculate WCRT
		\item[-] fragmentation and lost CPU time
		\item[-] complex schedule must be redone when program changes
	\end{itemize}

	\subsection{Dynamic Scheduling}
	\subsubsection{Rate Monotonic Scheduling}
	Priority-based scheduling with priority according to period. We assume an interrupt-driven fixed-priority scheduler:
	\begin{itemize}
		\item periodic threads
		\item no blocking
		\item deadline (D) = period (T)
		\item instantaneous context switch
	\end{itemize}
	Generally possible to guarantee schedulability if:
	\begin{align*}
		\sum &\frac{C_i}{T_i} < n(2^{1/n} - 1) \\
		\text{where } n &= \text{ number of threads} \\ 
		C_i &= \text{ execution time} \\ 
		T_i &= \text{ period}
	\end{align*}
	
	But it is sometimes possible to have schedulability above the limit, to check this look at what happens during the \textbf{critical instant}, when all threads are released at the same time, if they meet their first deadline, they will meet all subsequent ones. 

	We can calculate the worst case response time, $R$, of each thread:
	\begin{align*}
		&R_i^{k+1} = C_i + \sum_{j \in hp(i)} \ceil*{\frac{R_i^k}{T_j}}C_j \\
		\text{where } hp(i) &= \text{set of threads with higher priority than i} \\
		\ceil*{\frac{R_i^k}{T_j}} &= \text{number of times i is preempted by j} \\
		R_i^0 &= 0
	\end{align*}

	\subsubsection{Generalized RMS}
	We relax some of the assumptions from RMS to generalize it:
	\begin{itemize}
		\item non-periodic threads (more optimistic)
		\item blocking (on shared resources with priority inheritance)
		\item shorter deadlines (D < T)
		\item non-instantaneous context switch (clock interrupts, etc)
	\end{itemize}

	If we factor in blocking we add to our response time $B_i$, the sum of:
	\begin{description}
		\item[normal blocking] waiting for another thread to release a resource
		\item[push-through blocking] blocking resolved by priority inheritance
		\item[ceiling blocking] blocking because of priority ceiling
	\end{description}

	\subsubsection{Earliest Deadline First}
	Assign CPU to thread with the earliest deadline
	\begin{itemize}
		\item[+] 100\% CPU usage possible
		\item[-] expensive to implement
		\item[-] misses all deadlines at overload
	\end{itemize}

	\subsubsection{Run-Time Overhead}
	Real systems include:
	\begin{description}
		\item[release jitter] variations in time to release a thread
		\item[context switch] time to switch to another thread
		\item[clock interrupts] periodic interrupts driving preemption and context switches
	\end{description}

	We can get realistic measurements either through actual measurement, taking the worst case runtimes, or through formal analysis, but new ideas like pipelining and cacheing make this difficult
	\subsection{Blocking}
	If a lower priority thread holds a resource needed for a higher priority thread it can end up blocking it (against priority protocol), this is called \textbf{priority inversion}. We can solve this using a \textbf{priotiy inheritance protocol}:

	\subsubsection{Basic Priority Inheritance} 
	Temporarily raise the priority of low blocking thread to release the resource, locally for each resource

	Blocking time for thread $i$ is at most the minimum of the number of lower priority tasks that can block $i$ and the number of semaphores that can be used to block $i$

	\subsubsection{Priority Ceiling} 
	Allow only one low priority thread to access resources of high priority thread at any time, this allows us to avoid mulitple blockings for highest proirity thread at the expense of average blocking times and runtime overhead

	restrictions on \lstinline|lock| and \lstinline|unlock|:
	\begin{itemize}
		\item a task must release all resources between invocation
		\item computation time for task $i$ while holding semaphore $s$ is bounded to the length of the critical section for $s$
		\item a task may only lock semaphores from a fixed set known beforehand
	\end{itemize}

	If \lstinline|PC(s)| is the highest priority of a task that may lock semaphore $s$, and \lstinline|pri(i)| is the priority of task $i$ then at runtime:
	\begin{itemize}
		\item if task $i$ wants to lock $s$, it can only do so if \lstinline|pri(i) > PC(s)|
		\item else, $i$ is blocked and the task currently holding $s$ inherits the priority of $i$
	\end{itemize}


	This means that our code is deadlock free and blocking time for a task is at most the critical section of \textit{one} lower priority task

	\subsubsection{Immediate Inheritance} 
	Always raise the priority of a thread to the ceiling of the lock during its critical region

	\begin{itemize}
		\item same worst case time as priority ceiling
		\item easy to implement
		\item no need for block queues on single-processor
	\end{itemize}
	
	\subsection{Sporadic Threads}
	A thread triggered at unpredictable points in time

	We can pessimisticall model as a periodic thread with period equal to minimum time between events or take a more realistic approach with a \textbf{sporadic server}, a periodic thread that handles all sporadic jobs
	\begin{itemize}
		\item apply standard schedulability test
		\item does not assume a minimum time between events
		\item cannot guarantee that deadlines are met for sporadic events
	\end{itemize}


	\section{Real-Time Java}

	\subsection{Language Safety}
	With real-time development we want to manage:
	\begin{itemize}
		\item system complexity
		\item system development
	\end{itemize}

	For complexity, we use a multi-stage deploy process but unsafe languages can still hinder development process with hard to find errors:
	\begin{itemize}
		\item manual memory management
		\item type casting
		\item pointer arithmetic
		\item arrays without bound checking
	\end{itemize}
	Safe languages help make code correctness easier to guarantee:
	\begin{itemize}
		\item compiler checks
		\item automatic memory management
		\item errors leading to error messages
	\end{itemize}

	\subsection{Garbage Collection}
	\subsubsection{Background}
	Manual memory management can lead to problems with
	\begin{itemize}
		\item dangling pointers
		\item memory leaks
		\item fragmentation
	\end{itemize}

	\subsection{Reference Counting}
	Count the number of references to each object and deallocate when counter reaches 0
	\begin{itemize}
		\item[+] easy to implement, short pause
		\item[-] expensive (overhead)
		\item[-] no compaction
		\item[-] doesn't detect circular structures
	\end{itemize}

	\subsection{Traversing Algorithms}
	Periodically traverse pointer graph from root pointers outside heap, and delete objects not encountered in traversal

	Compacting can also be done simultaneously:
	\begin{description}
		\item[Mark-compact] determine where objects will be after compaction, update pointers, and slide objects into new position
		\item[Copying] alternate between using two subheaps, when current subheap is full, copy over all live objects into other empty subheap, compacting in the process
	\end{description}

	\subsubsection{Generation-based GC}
	Partition into several ``generations'' that are garbage collected separately
	\begin{itemize}
		\item young generations need to be checked most often, more efficient
		\item complex to keep track of inter-generation pointers
	\end{itemize}

	\subsubsection{Conservative GC}
	When we don't know runtime type information, we have to regard everything as a pointer, but we can't modify pointers which means no compaction

	\subsubsection{Incremental GC}
	``Stop-the-world'' pauses to do garbage collection, needs to be an incremental variant of a GC algorithm (can be easy as reference counting is incremental by nature)

	\subsubsection{Real-Time GC}
	Real-time adds another layer of complexity because we have soft and hard deadlines to meet and the GC can't interfere with them
	\begin{itemize}
		\item small number of periodic threads with high priority (hard requirements)
		\item large number of low priority threads (soft requirements)
	\end{itemize}

	Requirements:
	\begin{itemize}
		\item minimal response time and latency for high priority
		\item predictable (and low) worst case response time
		\item guaranteed schedulability
	\end{itemize}

	Idea:
	\begin{itemize}
		\item perform GC work in pauses between high priority executions
		\item low priority threads should do their own incremental GC
		\item interruptible GC, minimum locking
	\end{itemize}


\end{document}
